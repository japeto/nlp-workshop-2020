{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Base Population Using  Relation Extraction and Word2vec \n",
    "**by JAPeTo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Knowledge Base Population (KBP) is to automatically identifyrelevant entities, learn and disc discover attributes about the its relations, and finally search, expand the KB with other relations. \n",
    "\n",
    "The idea is take a small set of samples pairs. Automatically defining semantic relation and expand the set with new pairs.\n",
    "## Installation\n",
    "1. **Prerequisites**\n",
    "    You need to have these libraries.\n",
    "    * Python >= 3.0  \n",
    "    * [gensim](https://radimrehurek.com/gensim/) library\n",
    "    * *NumPy* and *SciPy* include in gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Setting paths**\n",
    "    Sample in [config.py](http://localhost:8888/edit/config.py) file:\n",
    "\n",
    "    * **word2vec_file** - Path to file with word embeddings dataset. \n",
    "    Yo could be use any format also by word2vec (vec or bin) or custom vectors from gensim library. \n",
    "    Popular pre-trained datasets can be found on official \n",
    "    [word2vec page](https://code.google.com/archive/p/word2vec/) as [Google News dataset](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) (1.5GB).\n",
    "\n",
    "    * **output_file** - New expand set of pairs (entities have a possible semantic relation) whitout tag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "config={}\n",
    "config[\"word2vec_file\"] = '/Users/macbookpro/Downloads/GoogleNews-vectors-negative300.bin'\n",
    "config[\"input_kbase\"] = '/Users/macbookpro/Desktop/nlp-workshop-2020/inputs/capitals.txt'\n",
    "curr_date = str(datetime.date.today())\n",
    "config[\"new_kbase\"] = f'/Users/macbookpro/Desktop/nlp-workshop-2020/outputs/output_{curr_date}.txt'\n",
    "\n",
    "\n",
    "# own libraries\n",
    "from utilities import *\n",
    "import embedd_utils as utils\n",
    "import classes as model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_object(word=None, vector=None):\n",
    "    \"\"\"\n",
    "    This method serves as a interface to embedding cache. If the embedding with given word was already\n",
    "    used it will return this object. Otherwise it will create new object with specified vector.\n",
    "    :param word: string\n",
    "    :param vector: list of floats\n",
    "    :return: Embedding\n",
    "    \"\"\"\n",
    "    if word is None:\n",
    "        return Embedding(vector=vector)\n",
    "    cached = utils.cached_embedding(word)\n",
    "    if cached is None:\n",
    "        utils.embeddings[word] = Embedding(word=word, vector=vector)\n",
    "    return utils.embeddings[word]\n",
    "\n",
    "def method_names(x):\n",
    "    return {\n",
    "        1: 'avg',\n",
    "        2: 'max',\n",
    "        3: 'svm'\n",
    "    }[x]\n",
    "\n",
    "def similarity_names(x):\n",
    "    return {\n",
    "        1: 'euclidean',\n",
    "        2: 'cosine',\n",
    "    }[x]\n",
    "\n",
    "def normalization_names(x):\n",
    "    return {\n",
    "        1: 'none',\n",
    "        2: 'standard',\n",
    "        3: 'softmax'\n",
    "    }[x]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word2Vec model\n",
    "Load pretrained model from google dataset, the model cannot be refined with additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model = utils.load_model(config[\"word2vec_file\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the recognizer\n",
    "From dataset build a **recognizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = model.PairSet.create_from_file(filename=config[\"input_kbase\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **recognizer** seek pair candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pairs = builder.find_new_pairs(output=config[\"new_kbase\"], result_count=3,neighborhood=5,\n",
    "                                      method=method_names(2),\n",
    "                                      distance=similarity_names(2),\n",
    "                                      normalization=normalization_names(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair List, samples and candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Samples\n",
      "\u001b[32mt\u001b[0m \u001b[34mAthens\u001b[0m \u001b[34mGreece\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mBaghdad\u001b[0m \u001b[34mIraq\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mBangkok\u001b[0m \u001b[34mThailand\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mBeijing\u001b[0m \u001b[34mChina\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mBerlin\u001b[0m \u001b[34mGermany\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mBern\u001b[0m \u001b[34mSwitzerland\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mCairo\u001b[0m \u001b[34mEgypt\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mCanberra\u001b[0m \u001b[34mAustralia\u001b[0m\n",
      "##### Candidates\n",
      "\u001b[31m?\u001b[0m \u001b[32mEgypt\u001b[0m \u001b[34mCairo\u001b[0m\n",
      "\u001b[31m?\u001b[0m \u001b[32mPakistan\u001b[0m \u001b[34mIslamabad\u001b[0m\n",
      "\u001b[31m?\u001b[0m \u001b[32mThailand\u001b[0m \u001b[34mBangkok\u001b[0m\n",
      "\u001b[31m?\u001b[0m \u001b[32mRussia\u001b[0m \u001b[34mMoscow\u001b[0m\n",
      "\u001b[31m?\u001b[0m \u001b[32mIran\u001b[0m \u001b[34mTehran\u001b[0m\n",
      "\u001b[31m?\u001b[0m \u001b[32mFrance\u001b[0m \u001b[34mParis\u001b[0m\n",
      "\u001b[31m?\u001b[0m \u001b[32mJapan\u001b[0m \u001b[34mTokyo\u001b[0m\n",
      "\u001b[31m?\u001b[0m \u001b[32mCuba\u001b[0m \u001b[34mHavana\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*5, \"Samples\")\n",
    "show_content_file(config[\"input_kbase\"], lines=8)\n",
    "\n",
    "print(\"#\"*5, \"Candidates\")\n",
    "new_pairs.print(lines=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "With candidates and samples train a svm **Classifier**\n",
    "\n",
    "Before of run build [SVM_pef](http://www.cs.cornell.edu/people/tj/svm_light/svm_perf.html)\n",
    "- Download\n",
    "- Compile\n",
    "- set folder bellow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://ipython-books.github.io/pages/chapter08_ml/05_svm_files/kernel.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://ipython-books.github.io/pages/chapter08_ml/05_svm_files/kernel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start SVM [svm_perf_learn] 1589166126\n",
      "load predictions from /Users/macbookpro/Desktop/nlp-workshop-2020/outputs/svm/svm_2020-05-10_capitals_prediction\n"
     ]
    }
   ],
   "source": [
    "config[\"svm_perf_path\"] = f' /Users/macbookpro/Desktop/nlp-workshop-2020/lib/svm_perf/'\n",
    "config[\"svm_folder\"] = f'/Users/macbookpro/Desktop/nlp-workshop-2020/outputs/svm/svm_{curr_date}'\n",
    "\n",
    "tagged = builder.svm_learning(method='svm', \n",
    "                              svm_folder= config[\"svm_folder\"], \n",
    "                              svm_perf_path= config[\"svm_perf_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.ResultList()\n",
    "results.from_array(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Samples\n",
      "\u001b[32mt\u001b[0m \u001b[34mAthens\u001b[0m \u001b[34mGreece\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mBaghdad\u001b[0m \u001b[34mIraq\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mBangkok\u001b[0m \u001b[34mThailand\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mBeijing\u001b[0m \u001b[34mChina\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mBerlin\u001b[0m \u001b[34mGermany\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mBern\u001b[0m \u001b[34mSwitzerland\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mCairo\u001b[0m \u001b[34mEgypt\u001b[0m\n",
      "\u001b[32mt\u001b[0m \u001b[34mCanberra\u001b[0m \u001b[34mAustralia\u001b[0m\n",
      "##### Candidates tagged\n",
      "\u001b[31mt\u001b[0m \u001b[32mSouth_Korea\u001b[0m \u001b[34mKorea\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mArgentine\u001b[0m \u001b[34mArgentina\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mPrague_Czech_Republic\u001b[0m \u001b[34mCzech_Republic\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mDoer\u001b[0m \u001b[34mStephen_Harper\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mHu\u001b[0m \u001b[34mWen\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mCanadians\u001b[0m \u001b[34mManitobans\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mSpanish\u001b[0m \u001b[34mPortuguese\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mYamada\u001b[0m \u001b[34mTanaka\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mBrisbane\u001b[0m \u001b[34mAdelaide\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mMunich_Germany\u001b[0m \u001b[34mGermany\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mVientiane\u001b[0m \u001b[34mLaos\u001b[0m\n",
      "\u001b[31mt\u001b[0m \u001b[32mEcuadorean\u001b[0m \u001b[34mEcuador\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*5, \"Samples\")\n",
    "show_content_file(config[\"input_kbase\"], lines=8)\n",
    "\n",
    "print(\"#\"*5, \"Candidates tagged\")\n",
    "[str(order) for order in results if str(order)[0] ==\"t\"]\n",
    "results.print(lines=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
